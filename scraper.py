import requests
import pandas as pd
import time
import os

# --- é…ç½®åŒº ---
BASE_URL = "https://cosylab.iiitd.edu.in/flavordb/entities_json?id="
SAVE_FILE = "flavordb_data.csv"
MAX_ID = 2600  # è®¾å®šä¸Šé™
BATCH_SIZE = 20 # æ¯20ä¸ªå­˜æ¡£ä¸€æ¬¡

def get_last_id():
    """æ£€æŸ¥è¿›åº¦ï¼šçœ‹çœ‹åˆ°åº•æŠ“åˆ°å“ªäº†"""
    if os.path.exists(SAVE_FILE):
        try:
            df = pd.read_csv(SAVE_FILE)
            if not df.empty:
                return int(df['id'].max())
        except:
            return 0
    return 0

def run_scraper():
    last_id = get_last_id()
    start_id = last_id + 1
    
    # åŠ è½½å·²æœ‰æ•°æ®
    if os.path.exists(SAVE_FILE):
        try:
            results = pd.read_csv(SAVE_FILE).to_dict('records')
        except:
            results = []
    else:
        results = []

    print(f"ğŸ”„ æ­£åœ¨æ£€æŸ¥æ–­ç‚¹... å·²å®Œæˆè‡³ ID {last_id}ã€‚å‡†å¤‡ä» {start_id} å¼€å§‹æ•æ‰ï¼")

    if start_id > MAX_ID:
        print("âœ¨ æ­å–œï¼å…¨é‡æ•°æ®å·²æŠ“å–å®Œæ¯•ã€‚")
        return

    for i in range(start_id, MAX_ID + 1):
        try:
            headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7)'}
            response = requests.get(f"{BASE_URL}{i}", timeout=15, headers=headers)
            
            if response.status_code == 200:
                data = response.json()
                molecules = data.get("molecules", [])
                
                # æå–ç»†èŠ‚
                flavor_set = set()
                molecule_names = []
                for m in molecules:
                    profiles = m.get("flavor_profile", "")
                    if profiles:
                        flavor_set.update(profiles.split("@"))
                    m_name = m.get("common_name")
                    if m_name:
                        molecule_names.append(m_name)
                
                # è¿™é‡Œå°±æ˜¯ä½ åˆšæ‰æŠ¥é”™çš„åœ°æ–¹ï¼Œè¿™æ¬¡æˆ‘å·²ç»å¸®ä½ å®Œæ•´é—­åˆäº†
                results.append({
                    "id": i,
                    "name": data.get("entity_alias_readable", "Unknown"),
                    "category": data.get("category_readable", "Unknown"),
                    "flavors": ", ".join(sorted(list(flavor_set))),
                    "molecules_count": len(molecules),
                    "sample_molecules": ", ".join(molecule_names[:10])
                })
                print(f"âœ… ID {i}: {data.get('entity_alias_readable', 'æœªçŸ¥')} | åˆ†å­æ•°: {len(molecules)}")
            
            elif response.status_code == 404:
                print(f"â© ID {i}: æ•°æ®åº“ç©ºç¼º (404)")
            
        except Exception as e:
            print(f"âŒ ID {i} å‘ç”Ÿæ•…éšœ: {e}")
            break 

        # è‡ªåŠ¨å­˜æ¡£
        if i % BATCH_SIZE == 0:
            pd.DataFrame(results).to_csv(SAVE_FILE, index=False)
            print(f"ğŸ’¾ è¿›åº¦å·²å®‰å…¨å­˜ç›˜ (ID {i})")
            time.sleep(1)

    pd.DataFrame(results).to_csv(SAVE_FILE, index=False)
    print(f"ğŸ æ•æ‰ä»»åŠ¡ç»“æŸã€‚")

if __name__ == "__main__":
    run_scraper()
